{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"EnCQR_with_LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GLbOfX-1EQy","outputId":"4fd81590-e5d4-4760-8c3a-9a66e5134405"},"source":["from google.colab import drive\n","drive.mount('/content/drive/',force_remount=True)\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import _pickle as pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras import backend as K, optimizers\n","from tensorflow.keras.layers import Activation, Dropout, Layer, Conv1D, Dense, BatchNormalization\n","from tqdm.notebook import trange\n","import time\n","import pandas as pd\n","from datetime import datetime, timedelta\n","from sklearn import preprocessing"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","metadata":{"id":"KmMheXLiY1eN"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"JwikraIDZKas"},"source":["def compute_coverage_len(y_test, y_lower, y_upper,verbose=False):\n","    \"\"\" \n","    Compute average coverage and length of prediction intervals\n","    \"\"\"\n","    in_the_range = np.sum((y_test >= y_lower) & (y_test <= y_upper))\n","    coverage = in_the_range / (y_test.shape[0]*y_test.shape[1])\n","    avg_length = np.mean(abs(y_upper - y_lower))\n","    avg_length = avg_length/(y_test.max()-y_test.min())\n","    if verbose==True:\n","      print(\"PI coverage:\",coverage,\",PI avg. length\",avg_length)\n","    return coverage, avg_length\n","\n","\n","def asym_nonconformity(label,low,high):\n","    \"\"\"\n","    Compute the asymetric conformity score\n","    \n","    credit - https://github.com/yromano/cqr\n","    \"\"\"\n","    y_lower = low\n","    y_upper = high\n","    error_high = label - y_upper \n","    error_low = y_lower - label\n","    return error_low, error_high\n","\n","\n","def transform_to_windows(data_org, station):\n","    \"\"\"\n","    Create dataframe with hours as columns \n","\n","    credit - https://github.com/nicholasjhana/short-term-energy-demand-forecasting\n","    \"\"\"\n","    #from the original datetime index create new columns with each of the year, month, day, and hour.\n","    data = data_org.copy()\n","    data.loc[:,'year'] = data.index.year\n","    data.loc[:,'month'] = data.index.month\n","    data.loc[:,'day'] = data.index.day\n","    data.loc[:,'hours'] = data.index.hour\n","    #construct datetimes from the split year, month, day columns\n","    data.loc[:,'date'] = pd.to_datetime(data.loc[:,['year', 'month', 'day']], format='%Y-%m-%d', errors='ignore')\n","    #set the index to dates only\n","    data = data.set_index(pd.DatetimeIndex(data['date']))\n","    #drop non target columns \n","    data = data.loc[:,[station, 'hours']]\n","    #pivot the table into the format Date h0, h1, ...h23\n","    data = data.pivot(columns='hours', values=station)\n","    data = data.dropna()\n","    return data\n","\n","\n","def split_sequences(sequences, n_steps):\n","    \"\"\"\n","    Split data into observations and labels \n","\n","    credit - https://github.com/nicholasjhana/short-term-energy-demand-forecasting\n","    \"\"\"\n","    max_step=n_steps\n","    n_steps+=1\n","    X, y = list(), list()\n","    for i in range(len(sequences)):\n","        # find the end of this pattern\n","        end_ix = i + max_step\n","        #create a list with the indexes we want to include in each sample\n","        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -7)]\n","        #reverse the slice indexes\n","        slices = list(reversed(slices))\n","        # check if we are beyond the dataset\n","        if end_ix > len(sequences)-1:\n","            break\n","        # gather input and output parts of the pattern\n","        seq_x = sequences[slices, :]\n","        seq_y = sequences[end_ix, :]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","    X = np.array(X)\n","    X = np.reshape(X,(X.shape[0],X.shape[1]*X.shape[2],1))\n","    y = np.array(y)\n","    return X, y\n","\n","\n","def normalize_data(data):\n","    if len(data.shape) == 3:\n","      data_r = data.reshape(-1,data.shape[1]*data.shape[-1])\n","      scaler = preprocessing.MinMaxScaler().fit(data_r)\n","      data_norm = scaler.transform(data_r).reshape(-1,data.shape[1],data.shape[-1])\n","    else:\n","      scaler = preprocessing.MinMaxScaler().fit(data)\n","      data_norm = scaler.transform(data)\n","    return data_norm, scaler\n","\n","\n","def create_datasets(data):\n","    \"\"\"\n","    Create input-output pairs from a dataframe with observations\n","    \"\"\"\n","    df_data=[]\n","    cols = data.columns\n","    for i,s in enumerate(cols): \n","        if i == 0:\n","            X,Y = split_sequences(transform_to_windows(data,s).values,7)\n","        else:\n","            x,y = split_sequences(transform_to_windows(data,s).values,7)\n","            X = np.append(X,x,-1)\n","    return X,Y\n","\n","\n","\n","def create_ensemble_datasets(df_train,B):\n","  train=[]\n","  train_ensemble=[]\n","  for j,s in enumerate(df_train.columns):\n","    df_t = transform_to_windows(df_train,s)\n","    X_full,Y_full = split_sequences(df_t.values,7)\n","    X_norm, scaler_x = normalize_data(X_full)\n","    Y_norm, scaler_y = normalize_data(Y_full)\n","    # train.append([X_norm,Y_norm,scaler_x,scaler_y])\n","    train_s = []\n","    for i in range(B):\n","      sb_size = int(np.floor(len(df_t)/B))\n","      X,Y = split_sequences(df_t[i*sb_size:i*sb_size+sb_size].values,7)\n","      X_r = X.reshape(X.shape[0],X.shape[1]*X.shape[2])\n","      X = scaler_x.transform(X_r)\n","      X = X.reshape((X.shape[0],X_full.shape[1],X_full.shape[2]))\n","      Y = scaler_y.transform(Y)\n","      train_s.append([X,Y])\n","    train_ensemble.append(train_s)\n","    # combine all features \n","  for j in range(B):\n","      for i in range(len(df_train.columns)):\n","          if i == 0:\n","              etrain_x, etrain_y= train_ensemble[i][j] \n","          else:\n","              etrain_xx, etrain_yy = train_ensemble[i][j]\n","              etrain_x = np.append(etrain_x,etrain_xx,axis=-1)\n","      train.append([etrain_x,etrain_y])\n","  return train\n","\n","\n","\n","class LSTM_stateful(keras.Model):\n","    def __init__(self, parameters):\n","      \"\"\" Initialization\n","      Parameters\n","      ---------\n","      parameters : list containing tcn network parameters\n","      \"\"\"\n","      super(LSTM_stateful, self).__init__()\n","      self.model_type = 'lstm'\n","      self.n_lags = parameters['n_lags']\n","      self.n_feat = parameters['n_feat']\n","      self.cells = parameters['cells']\n","      self.n_layers = parameters['n_layers']\n","      self.quantiles = parameters['quantiles']\n","      self.l2_l = parameters['l2']\n","      self.batch_s = parameters['batch_s']\n","      self.target_coverage = (self.quantiles[-1]-self.quantiles[0])\n","      self.num_quantiles = len(self.quantiles)\n","      self.lstm = keras.models.Sequential(name='lstm_stateful')\n","      \n","      with K.name_scope(self.name): \n","        if self.n_layers > 1:   \n","              for i in range(self.n_layers-1):\n","               self.lstm.add(keras.layers.LSTM(self.cells,stateful=True, return_sequences=True, kernel_regularizer=keras.regularizers.l2(self.l2_l)))\n","        \n","        self.lstm.add(keras.layers.LSTM(self.cells, stateful=True, kernel_regularizer=keras.regularizers.l2(self.l2_l)))\n","        self.lstm.add(keras.layers.Dense(24, kernel_regularizer=keras.regularizers.l2(self.l2_l)))\n","        self.out_quantiles = keras.layers.Dense(len(self.quantiles), kernel_regularizer=keras.regularizers.l2(self.l2_l))\n","\n","    def call(self,x,training=None): \n","      output = self.lstm(x) \n","      output = tf.reshape(output,(output.shape[0],output.shape[1],1))\n","      output_quantiles = self.out_quantiles(output)\n","      return output_quantiles\n","\n","\n","def predict_stateful(obs,lab,model,batch_size,pad=False):\n","  # stateful\n","  max_batch_count_val = int(obs.shape[0] / batch_size)\n","  X_test = obs[0:max_batch_count_val*batch_size]\n","  y_test = lab[0:max_batch_count_val*batch_size]\n","  missing = batch_size - (obs.shape[0] - max_batch_count_val*batch_size)\n","  # pad with zeros \n","  if pad == True:\n","    zero_pad_x = np.zeros((missing,obs.shape[1],obs.shape[2]))\n","    zero_pad_y = np.zeros((missing,lab.shape[1]))\n","    X_test = obs\n","    y_test = lab\n","    X_test = np.append(np.array(X_test),zero_pad_x,axis=0)\n","    y_test = np.append(np.array(y_test),zero_pad_y,axis=0)\n","  val_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n","  v_pred = []\n","  val_iter = val_data.batch(batch_size)\n","  for batch, (v_data,v_label) in enumerate(val_iter):\n","    valid_pred = model(v_data,training=False)\n","    v_pred.append(valid_pred)\n","  for p in range(len(v_pred)):\n","    if p == 0:\n","      va_pred = np.array(v_pred[p])\n","    else:\n","      va_pred = np.append(va_pred,v_pred[p],axis=0)\n","  if pad == True:\n","    va_pred = np.delete(va_pred,slice(len(va_pred)-missing,len(va_pred),1),axis=0)\n","  return va_pred\n","\n","\n","def train_model(model, train_x, train_y, test_x,test_y, trainer_params_list,early_stop = False):\n","    \"\"\" fit the model to the data\n","    Parameters:\n","    -----------\n","    model : network\n","    train_x : training obs\n","    train_y : training labels\n","    test_x : test obs\n","    test_y : test labels\n","    trainer_params_list : list of training parameters\n","\n","    Returns:\n","    --------\n","    model : trained network\n","    \"\"\"\n","    batch_size = trainer_params_list['batch_size']\n","    epochs = trainer_params_list['epoch_num']\n","    loss_func = trainer_params_list['loss_function']\n","    optimizer = trainer_params_list['optimizer']\n","    lr = trainer_params_list['lr']\n","\n","    # early stopping par\n","    patience = trainer_params_list['patience']\n","    delta = trainer_params_list['delta']\n","\n","    best_epoch = 0\n","    best_avg_length = 1e10\n","    best_coverage = 0\n","\n","    train_loss, test_loss,test_cov,test_len = [],[],[],[]\n","    max_batch_count = int(train_x.shape[0] / batch_size)\n","    max_batch_count_val = int(test_x.shape[0] / batch_size)\n","    missing = batch_size - (train_x.shape[0] - max_batch_count*batch_size)\n","\n","    X_train = train_x[0:max_batch_count*batch_size]\n","    y_train = train_y[0:max_batch_count*batch_size]\n","    X_test = test_x[0:max_batch_count_val*batch_size]\n","    y_test = test_y[0:max_batch_count_val*batch_size]\n","\n","    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","    val_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n","\n","    ### The training process\n","    for epoch in trange(epochs):\n","        start=time.time()\n","        epoch_loss_avg = tf.keras.metrics.Mean()\n","        epoch_loss_avg_test = tf.keras.metrics.Mean()\n","\n","        train_iter = train_data.batch(batch_size)\n","        val_iter = val_data.batch(batch_size)\n","        for batch, (conv_data,label) in enumerate(train_iter):\n","            with tf.GradientTape() as tape:\n","                output= model(conv_data, training=True) \n","                loss = loss_func(label,output)\n","            grad = tape.gradient(loss,model.trainable_variables)\n","            optimizer.apply_gradients(zip(grad, model.trainable_variables))\n","            epoch_loss_avg.update_state(loss)\n","        train_loss.append(epoch_loss_avg.result())\n","\n","        # evaluation\n","        v_pred = []\n","        for batch, (v_data,v_label) in enumerate(val_iter):\n","          valid_pred = model(v_data,training=False)\n","          v_pred.append(valid_pred)\n","          val_loss = loss_func(v_label,valid_pred)\n","          epoch_loss_avg_test.update_state(val_loss)\n","        test_loss.append(epoch_loss_avg_test.result())\n","        for p in range(len(v_pred)):\n","          if p == 0:\n","            va_pred = np.array(v_pred[p])\n","          else:\n","            va_pred = np.append(va_pred,v_pred[p],axis=0)\n","        coverage, avg_length = compute_coverage_len(y_test,va_pred[:,:,0],va_pred[:,:,-1])\n","        test_cov.append(coverage)\n","        test_len.append(avg_length)\n","        if (coverage >= model.target_coverage) and (avg_length < best_avg_length):\n","          best_avg_length = avg_length\n","          best_coverage = coverage\n","          best_epoch = epoch\n","\n","        # EarlyStopping\n","        if early_stop == True:\n","          if len(test_loss) > patience:\n","              last_patience_epochs = [x + delta for x in test_loss[::-1][1:patience + 1]]\n","              current_metric = test_loss[::-1][0]\n","              if current_metric >= min(last_patience_epochs):\n","                break\n","\n","    # plotting learning curves\n","    plt.figure()\n","    plt.plot(train_loss,label = 'train loss')\n","    plt.plot(test_loss,label = 'test loss')\n","    plt.legend()\n","    plt.show()\n","    if len(model.quantiles) >= 2:\n","      plt.figure()\n","      plt.plot(test_cov,label = 'test cov')\n","      plt.plot(test_len,label = 'test len')\n","      plt.hlines(model.target_coverage,0,len(test_cov))\n","      plt.ylim((0,1.5))\n","      plt.legend()\n","      plt.show()\n","    return model\n","\n","class PinballLoss(keras.Model):\n","  \"\"\" Pinball loss for multiple quantiles\n","  \"\"\"\n","  def __init__(self,quantiles):\n","    super(PinballLoss,self).__init__()\n","    self.quantiles = quantiles\n","\n","  def call(self,labels,pred):\n","    \"\"\" Compute pinball loss between labels and pred \n","    Parameters:\n","    ----------\n","    labels : true labels\n","    pred : predictions of true labels \n","\n","    Returns:\n","    -------\n","    total_loss : pinball loss value for all quantiles combined \n","    \"\"\"\n","    assert len(labels) == len(pred)\n","    loss = []\n","    for i,q in enumerate(self.quantiles):\n","      error = tf.subtract(labels,pred[:,:,i])\n","      loss_q = tf.reduce_mean(tf.maximum(q*error,(q-1)*error))\n","      loss.append(loss_q)\n","    L = tf.convert_to_tensor(loss)\n","    total_loss = tf.reduce_mean(L)\n","    return total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLjGm92pZGGD"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"g9cVictxaJf8"},"source":["url = 'https://raw.githubusercontent.com/Duvey314/austin-green-energy-predictor/master/Resources/Output/Webberville_Solar_2017-2020_MWH.csv'\n","df = pd.read_csv(url)\n","df = df.drop(columns=('Weather_Description'))\n","df = df.drop(columns=('Year'))\n","df = df.drop(columns=('Month'))\n","df = df.drop(columns=('Day'))\n","df = df.drop(columns=('Hour'))\n","df = df.drop(columns=('Date_Time'))\n","\n","# create date+hour index \n","date_list = pd.date_range(start='01/01/2017', end='31/07/2020')\n","date_list = pd.to_datetime(date_list)\n","hour_list = []\n","for nDate in date_list:\n","    for nHour in range(24):\n","        tmp_timestamp = nDate+timedelta(hours=nHour)\n","        hour_list.append(tmp_timestamp)\n","date_list = pd.to_datetime(hour_list) \n","\n","df['hour_list'] = date_list[:-1]\n","df = df.set_index('hour_list')\n","\n","# train, val, test datasets\n","df_train = df[0:365*24]\n","df_val = df[365*24:365*24*2]\n","df_test = df[365*24*2:365*24*3]\n","df_test_y = df_test[168:]\n","# split into input-output pairs\n","train_x, train_y = create_datasets(df_train)\n","val_x, val_y = create_datasets(df_val)\n","val_x, scaler_val_x = normalize_data(val_x)\n","val_y, scaler_val_y =normalize_data(val_y)\n","test_x, test_y = create_datasets(df_test)\n","test_x, scaler_test_x = normalize_data(test_x)\n","test_y, scaler_test_y = normalize_data(test_y)\n","\n","# training data labels scaler \n","scaler_train_y = preprocessing.MinMaxScaler().fit(train_y)\n","\n","# create ensemble subsets - subsamping\n","df_ensemble = create_ensemble_datasets(df_train,B=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uTQB5erhAvaf"},"source":["# EnCQR LSTM"]},{"cell_type":"code","metadata":{"id":"XOlZOufC6ra1"},"source":["def fit_EnCQR_models(train, testX, testY, trainer_list, B, index, alpha):\n","  \"\"\" fit B models on B subsets created from train data\n","  Parameters\n","  ----------\n","  trainX - training obs.\n","  trainY - training labels\n","  trainer_list - network/training parameters\n","  B      - no. ensemble models \n","\n","  Returns\n","  -------\n","  f_hat_b_agg     - aggregated leave-one-out predictions for Sb\n","  ensemble_models - trained models\n","  indx            - array of Sb indices\n","  \"\"\"\n","  f_hat_b_agg_low = np.zeros((len(train[index[0]][0]),24,B))\n","  f_hat_b_agg_mean = np.zeros((len(train[index[0]][0]),24,B))\n","  f_hat_b_agg_high = np.zeros((len(train[index[0]][0]),24,B))\n","  ensemble_models = []\n","\n","  # dict containing LOO predictions\n","  dct_lo = {}\n","  dct_mean = {}\n","  dct_hi = {}\n","  for key in index:\n","    dct_lo['pred_%s' % key] = []\n","    dct_mean['pred_%s' % key] = []\n","    dct_hi['pred_%s' % key] = []\n","\n","  # training a model for each sub set Sb\n","  for b in range(B):\n","    f_hat_b = LSTM_stateful(trainer_list['parameters'])\n","    f_hat_b = train_model(f_hat_b,train[index[b]][0],train[index[b]][1], testX, testY, trainer_list,early_stop=True)\n","    ensemble_models.append(f_hat_b)\n","    # Leave-one-out predictions for each Sb\n","    indx_LOO = index[np.arange(len(index))!=b]\n","    for i in range(len(indx_LOO)):\n","        pred = predict_stateful(train[indx_LOO[i]][0],train[indx_LOO[i]][1],f_hat_b,f_hat_b.batch_s,pad=True)\n","        pred_inv = np.zeros((pred.shape[0],pred.shape[1],pred.shape[-1]))\n","        for j in range(pred_inv.shape[-1]):\n","          pred_inv[:,:,j] = trainer_list['scaler'].inverse_transform(pred[:,:,j])\n","        dct_lo['pred_%s' %indx_LOO[i]].append(pred_inv[:,:,0])\n","        dct_mean['pred_%s' %indx_LOO[i]].append(pred_inv[:,:,1])\n","        dct_hi['pred_%s' %indx_LOO[i]].append(pred_inv[:,:,-1])\n","  for b in range(B):\n","    f_hat_b_agg_low[:,:,b] = np.mean(dct_lo['pred_%s' %b],axis=0) \n","    f_hat_b_agg_mean[:,:,b] = np.mean(dct_mean['pred_%s' %b],axis=0) \n","    f_hat_b_agg_high[:,:,b] = np.mean(dct_hi['pred_%s' %b],axis=0)  \n","\n","  # residuals\n","  epsilon = []\n","  epsilon_hi=[]\n","  for j in range(B):\n","    trainY_inv = train[j][1] \n","    for i in range(trainY_inv.shape[0]):\n","      e_low,e_high = asym_nonconformity(label=trainY_inv[i,:],low=f_hat_b_agg_low[i,:,j],high=f_hat_b_agg_high[i,:,j])\n","      epsilon.append(e_low)\n","      epsilon_hi.append(e_high)\n","  epsilon = np.ndarray.flatten(np.array(epsilon))\n","  epsilon_hi = np.ndarray.flatten(np.array(epsilon_hi))\n","\n","  # predict test data\n","  f_hat_t_low = np.zeros((testX.shape[0],testY.shape[1],B))\n","  f_hat_t_mean = np.zeros((testX.shape[0],testY.shape[1],B))\n","  f_hat_t_high = np.zeros((testX.shape[0],testY.shape[1],B))\n","  for b,model in enumerate(ensemble_models):\n","   test_pred = predict_stateful(testX,testY,model,model.batch_s,pad=True)\n","  f_hat_t_low[:,:,b] = trainer_list['scaler'].inverse_transform(test_pred[:,:,0])\n","  f_hat_t_mean[:,:,b] = trainer_list['scaler'].inverse_transform(test_pred[:,:,1])\n","  f_hat_t_high[:,:,b] = trainer_list['scaler'].inverse_transform(test_pred[:,:,-1])\n","\n","  # construct PI\n","  prediction_interval = np.zeros((testY.shape[0],testY.shape[1],3))\n","  f_hat_t_agg_low = np.zeros((testY.shape[0],testY.shape[1]))\n","  f_hat_t_agg_mean = np.zeros((testY.shape[0],testY.shape[1]))\n","  f_hat_t_agg_hi = np.zeros((testY.shape[0],testY.shape[1]))\n","  org_prediction_interval = np.zeros((testY.shape[0],testY.shape[1],3))\n","\n","  # aggregate all B predictions for test data\n","  for day in range(testY.shape[0]):   \n","    f_hat_t_agg_low[day,:] = np.mean(f_hat_t_low[day,:,:],axis=-1)\n","    f_hat_t_agg_mean[day,:] = np.mean(f_hat_t_mean[day,:,:],axis=-1)\n","    f_hat_t_agg_hi[day,:] = np.mean(f_hat_t_high[day,:,:],axis=-1)\n","\n","    for hour in range(testY.shape[1]):\n","      # asymetric conformity score index\n","      index = int(np.ceil((1 - alpha / 2) * (len(epsilon) + 1))) - 1\n","      index = min(max(index, 0), len(epsilon) - 1)\n","\n","      # aggregate predictions\n","      f_quantile_low = np.mean(f_hat_t_low[day,hour,:])\n","      f_quantile_mean = np.mean(f_hat_t_mean[day,hour,:])\n","      f_quantile_high = np.mean(f_hat_t_high[day,hour,:])\n","\n","      # original PI, before conformalization \n","      org_prediction_interval[day,hour,0],org_prediction_interval[day,hour,1],org_prediction_interval[day,hour,-1] = f_quantile_low,f_quantile_mean,f_quantile_high\n","\n","      # conformalization\n","      e_quantile_lo = np.sort(epsilon)\n","      e_quantile_hi = np.sort(epsilon_hi)\n","      e_quantile_lo = e_quantile_lo[index]\n","      e_quantile_hi = e_quantile_hi[index]  \n","      prediction_interval[day,hour,0] = f_quantile_low - e_quantile_lo\n","      prediction_interval[day,hour,1] = f_quantile_mean\n","      prediction_interval[day,hour,-1] = f_quantile_high + e_quantile_hi\n","\n","    # update epsilon (s = 24) - TODO: include s as an input variable\n","    e_lo,e_hi = asym_nonconformity(label=testY[day,:],low=f_hat_t_agg_low[day,:],high=f_hat_t_agg_hi[day,:])\n","    epsilon = np.delete(epsilon,slice(0,24,1))\n","    epsilon_hi = np.delete(epsilon_hi,slice(0,24,1))\n","    epsilon = np.append(epsilon,e_lo)\n","    epsilon_hi = np.append(epsilon_hi,e_hi)\n","  return prediction_interval\n","\n","\n","### The model parameters  - LSTM\n","n_feat = 6\n","cells = 89\n","n_lags = 168\n","quantiles = [0.09,0.5,0.89]\n","n_layers = 1\n","l2_lambda = 0.005\n","statef = True\n","batch_size = 7\n","parameters = {'cells':cells,'n_feat':n_feat,'n_lags':n_lags,'quantiles':quantiles,'n_layers':n_layers,'l2':l2_lambda,'statef':statef,'batch_s':batch_size}\n","\n","### The taining parameters\n","n_epochs=100\n","lr = 0.0009 \n","optimizer = keras.optimizers.Adam(learning_rate = lr)\n","loss_function = PinballLoss(quantiles)\n","trainer_params_list = {'parameters':parameters,'batch_size': batch_size,'epoch_num':n_epochs, 'optimizer': optimizer,'loss_function':loss_function,'lr':lr,'patience':10,'delta':0.01,'scaler':scaler_train_y}\n","\n","# PREDICT\n","pred = fit_EnCQR_models(df_ensemble, test_x, test_y, trainer_params_list, 3, np.array([0,1,2]), 0.10)\n","df_pred = pd.DataFrame(data=pred.reshape(pred.shape[0]*pred.shape[1],pred.shape[-1]),columns=['0.05','0.50','0.95'],index=df_test[168:].index)\n","c,l = compute_coverage_len(test_y,pred[:,:,0],pred[:,:,-1],verbose=True)\n","plt.rcParams[\"figure.figsize\"] = (20,5)\n","df_pred[1000:1168].plot()\n","df_test_y['MWH'][1000:1168].plot()"],"execution_count":null,"outputs":[]}]}